{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# U-Netを操作するクラス,関数群\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LeakyReLU, BatchNormalization, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv2D, ZeroPadding2D, Conv2DTranspose\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "\n",
    "# imageは(256, 256, 1)で読み込み\n",
    "IMAGE_SIZE = 256\n",
    "# 一番初めのConvolutionフィルタ枚数は32\n",
    "FIRST_LAYER_FILTER_COUNT = 64\n",
    "\n",
    "# convolution後のshape_size=(size_old-filter_size)/stride+1\n",
    "\n",
    "\n",
    "# U-Netのネットワークを構築するクラス\n",
    "class UNet(object):\n",
    "    def __init__(self, input_channel_count, output_channel_count, first_layer_filter_count):\n",
    "        # 以下,first_layer_filter_count:Nと表記\n",
    "        self.INPUT_IMAGE_SIZE = IMAGE_SIZE\n",
    "        self.CONCATENATE_AXIS = -1\n",
    "        # チャンネルの軸で結合することを指定している\n",
    "        self.CONV_FILTER_SIZE = 4\n",
    "        self.CONV_STRIDE = 2\n",
    "        self.CONV_PADDING = (1, 1)\n",
    "        self.DECONV_FILTER_SIZE = 2\n",
    "        self.DECONV_STRIDE = 2\n",
    "\n",
    "        # build NN with functional API\n",
    "        input_img = Input((self.INPUT_IMAGE_SIZE, self.INPUT_IMAGE_SIZE, input_channel_count))\n",
    "        # (256, 256, input_channel_count)\n",
    "\n",
    "        enc1 = ZeroPadding2D(self.CONV_PADDING)(input_img)\n",
    "        # (258, 258, input_channel_count)\n",
    "\n",
    "        enc1 = Conv2D(first_layer_filter_count, self.CONV_FILTER_SIZE, strides=self.CONV_STRIDE)(enc1)\n",
    "        # (128, 128, N)\n",
    "\n",
    "        filter_count = first_layer_filter_count*2\n",
    "        enc2 = self._add_encoding_layer(filter_count, enc1)\n",
    "        # (64, 64, 2N)\n",
    "\n",
    "        filter_count = first_layer_filter_count*4\n",
    "        enc3 = self._add_encoding_layer(filter_count, enc2)\n",
    "        # (32, 32, 4N)\n",
    "\n",
    "        filter_count = first_layer_filter_count*8\n",
    "        enc4 = self._add_encoding_layer(filter_count, enc3)\n",
    "        # (16, 16, 8N)\n",
    "\n",
    "        enc5 = self._add_encoding_layer(filter_count, enc4)\n",
    "        # (8, 8, 8N)\n",
    "\n",
    "        dec4 = self._add_decoding_layer(filter_count, True, enc5)\n",
    "        # (16, 16, 8N)\n",
    "\n",
    "        dec4 = concatenate([dec4, enc4], axis=self.CONCATENATE_AXIS)\n",
    "        # (16, 16, 16N)\n",
    "\n",
    "        filter_count = first_layer_filter_count*4\n",
    "        dec5 = self._add_decoding_layer(filter_count, True, dec4)\n",
    "        # (32, 32, 4N)\n",
    "\n",
    "        dec5 = concatenate([dec5, enc3], axis=self.CONCATENATE_AXIS)\n",
    "        # (32, 32, 8N)\n",
    "\n",
    "        filter_count = first_layer_filter_count*2\n",
    "        dec6 = self._add_decoding_layer(filter_count, True, dec5)\n",
    "        # (64, 64, 2N)\n",
    "\n",
    "        dec6 = concatenate([dec6, enc2], axis=self.CONCATENATE_AXIS)\n",
    "        # (64, 64, 4N)\n",
    "\n",
    "        filter_count = first_layer_filter_count\n",
    "        dec7 = self._add_decoding_layer(filter_count, True, dec6)\n",
    "        # (128, 128, N)\n",
    "\n",
    "        dec7 = concatenate([dec7, enc1], axis=self.CONCATENATE_AXIS)\n",
    "        # (128, 128 ,2N)\n",
    "\n",
    "        dec8 = Activation(activation='relu')(dec7)\n",
    "        dec8 = Conv2DTranspose(output_channel_count, self.DECONV_FILTER_SIZE, strides=self.DECONV_STRIDE)(dec8)\n",
    "        # (256, 256, output_channel_count)\n",
    "\n",
    "        dec8 = Activation(activation='sigmoid')(dec8)\n",
    "\n",
    "        self.UNet = Model(inputs=input_img, outputs=dec8)\n",
    "\n",
    "        # self.UNet.summary()\n",
    "\n",
    "    def _add_encoding_layer(self, filter_count, sequence):\n",
    "        new_sequence = LeakyReLU(0.2)(sequence)\n",
    "        new_sequence = ZeroPadding2D(self.CONV_PADDING)(new_sequence)\n",
    "        new_sequence = Conv2D(filter_count, self.CONV_FILTER_SIZE, strides=self.CONV_STRIDE)(new_sequence)\n",
    "        new_sequence = BatchNormalization()(new_sequence)\n",
    "        return new_sequence\n",
    "\n",
    "    def _add_decoding_layer(self, filter_count, add_drop_layer, sequence):\n",
    "        new_sequence = Activation(activation='relu')(sequence)\n",
    "        new_sequence = Conv2DTranspose(filter_count, self.DECONV_FILTER_SIZE, strides=self.DECONV_STRIDE,\n",
    "                                       kernel_initializer='he_uniform')(new_sequence)\n",
    "        new_sequence = BatchNormalization()(new_sequence)\n",
    "        if add_drop_layer:\n",
    "            new_sequence = Dropout(0.5)(new_sequence)\n",
    "        return new_sequence\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.UNet\n",
    "\n",
    "\n",
    "# U-Netをtrainingする関数\n",
    "def train_unet():\n",
    "    # 訓練用imageデータ読み込み\n",
    "    x_train, file_names = load_x('Dataset' + os.sep + 'training' + os.sep + 'image')\n",
    "    # 訓練用labelデータ読み込み\n",
    "    y_train = load_y('Dataset' + os.sep + 'training' + os.sep + 'label')\n",
    "    # 検証用imageデータ読み込み\n",
    "    x_validation, file_names2 = load_x('Dataset' + os.sep + 'test' + os.sep + 'image')\n",
    "    # 検証用labelデータ読み込み\n",
    "    y_validation = load_y('Dataset' + os.sep + 'test' + os.sep + 'label')\n",
    "\n",
    "    # 入力はグレースケール1チャンネル\n",
    "    input_channel_count = 1\n",
    "    # 出力はグレースケール1チャンネル\n",
    "    output_channel_count = 1\n",
    "\n",
    "    # U-Netの生成\n",
    "    network = UNet(input_channel_count, output_channel_count, FIRST_LAYER_FILTER_COUNT)\n",
    "    model = network.get_model()\n",
    "    model.compile(loss=dice_coefficient_loss, optimizer=Adam(lr=1e-4), metrics=[dice_coefficient, 'accuracy'])\n",
    "\n",
    "    BATCH_SIZE = 5\n",
    "    # 20エポック回せば十分\n",
    "    NUM_EPOCH = 1000\n",
    "    history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCH, verbose=1,\n",
    "                        validation_data=(x_validation, y_validation))\n",
    "    model.save_weights('unet_weights.hdf5')\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習推移をプロットする関数群\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# historyを受け取りloss,accuracyの推移グラフを出力する関数\n",
    "def plot_loss_accuracy(history):\n",
    "    fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10, 4))\n",
    "\n",
    "    # Plot the loss in the history\n",
    "    axL.plot(history.history['loss'], label=\"loss for training\")\n",
    "    axL.plot(history.history['val_loss'], label=\"loss for validation\")\n",
    "    x = history.history['val_loss']\n",
    "    y = history.history['loss']\n",
    "\n",
    "    axL.set_title('model loss')\n",
    "    axL.set_xlabel('epoch')\n",
    "    axL.set_ylabel('loss')\n",
    "    axL.legend(loc='upper right')\n",
    "\n",
    "    # Plot the accuracy in the history\n",
    "    axR.plot(history.history['acc'], label=\"loss for training\")\n",
    "    axR.plot(history.history['val_acc'], label=\"loss for validation\")\n",
    "    axR.set_title('model accuracy')\n",
    "    axR.set_xlabel('epoch')\n",
    "    axR.set_ylabel('accuracy')\n",
    "    axR.legend(loc='lower right')\n",
    "\n",
    "    # グラフを保存\n",
    "    plt.show()\n",
    "    fig.savefig('./loss_accuracy.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metrics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lossの計算関数群\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "# ダイス係数を計算する関数\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    y_true = K.flatten(y_true)\n",
    "    y_pred = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true * y_pred)\n",
    "    return 2.0 * intersection / (K.sum(y_true) + K.sum(y_pred) + 1)\n",
    "\n",
    "\n",
    "# ロス関数\n",
    "def dice_coefficient_loss(y_true, y_pred):\n",
    "    return 1.0 - dice_coefficient(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像をロードする関数群\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 受け取ったパス下のファイル/ディレクトリのうち,'.DS_Store'以外のファイルのlistを返す関数\n",
    "def load_file(folder_path):\n",
    "    import os\n",
    "\n",
    "    file_list = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if os.path.isfile(os.path.join(folder_path, filename)) and not filename.startswith('.'):\n",
    "            file_list.append(filename)\n",
    "    return file_list\n",
    "\n",
    "\n",
    "# 受け取ったパス下の静脈画像をグレースケールで読み込み,ファイル名とセットで返す関数\n",
    "def load_x(folder_path):\n",
    "    import os\n",
    "    import cv2\n",
    "\n",
    "    file_names = load_file(folder_path)\n",
    "    file_names.sort()\n",
    "    images = np.zeros((len(file_names), IMAGE_SIZE, IMAGE_SIZE, 1), np.float32)\n",
    "    for i, image_file in enumerate(file_names):\n",
    "        image = cv2.imread(folder_path + os.sep + image_file, cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "        image = image[:, :, np.newaxis]\n",
    "        images[i] = normalize_x(image)\n",
    "    return images, file_names\n",
    "\n",
    "\n",
    "# ラベル画像をグレースケールで読み込んで返す関数\n",
    "def load_y(folder_path):\n",
    "    import os\n",
    "    import cv2\n",
    "\n",
    "    image_files = load_file(folder_path)\n",
    "    image_files.sort()\n",
    "    images = np.zeros((len(image_files), IMAGE_SIZE, IMAGE_SIZE, 1), np.float32)\n",
    "    for i, image_file in enumerate(image_files):\n",
    "        image = cv2.imread(folder_path + os.sep + image_file, cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "        image = image[:, :, np.newaxis]\n",
    "        images[i] = normalize_y(image)\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalize.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画素値を正規化/非正規化する関数群\n",
    "\n",
    "\n",
    "# 値を-1から1に正規化する関数\n",
    "def normalize_x(image):\n",
    "    image = image/127.5 - 1\n",
    "    return image\n",
    "\n",
    "\n",
    "# 値を0から1に正規化する関数\n",
    "def normalize_y(image):\n",
    "    image = image/255\n",
    "    return image\n",
    "\n",
    "\n",
    "# 値を0から255に戻す関数\n",
    "def denormalize_y(image):\n",
    "    image = image*255\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# masking.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous : 0.04752335765145042\n",
      "U-Net : 0.05934313210574063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# masking関連の関数\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# タイマーを定義したクラス\n",
    "import time\n",
    "\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self.start = time.time()\n",
    "\n",
    "    def time_elapsed(self):\n",
    "        return time.time() - self.start\n",
    "\n",
    "    def reset(self):\n",
    "        self.start = time.time()\n",
    "\n",
    "        \n",
    "# 林さんの従来手法でmaskingを行って返す関数\n",
    "def previous_masking(image):\n",
    "    tmp_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    timer1 = Timer()\n",
    "    tmp_image = cv2.morphologyEx(tmp_image, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 13)),\n",
    "                                 iterations=8)\n",
    "\n",
    "    ret, mask = cv2.threshold(tmp_image, 0, 255,  cv2.THRESH_BINARY|cv2.THRESH_OTSU)\n",
    "\n",
    "    masked = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "    return masked, timer1.time_elapsed()\n",
    "        \n",
    "\n",
    "# 動画でU-Netによるmaskingを行う関数\n",
    "def movie_masking(name):\n",
    "    user_movies = {'hayashi':'./movie/hayashi_n4.avi',\n",
    "                   'kikuchi':'./movie/kikuchi_n2.avi',\n",
    "                   'kurose':'./movie/kurose_n1.avi',\n",
    "                   'okazawa':'./movie/okazawa8.avi',\n",
    "                   'hayashi2':'./movie/hayashi/hayashi_%04d.png'}\n",
    "\n",
    "    cap = cv2.VideoCapture(user_movies[name])\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "    fps = 15\n",
    "    width = 1024\n",
    "    height = 768\n",
    "    writer = cv2.VideoWriter(name+'.mov', fourcc, fps, (width, int(height/2)))\n",
    "\n",
    "    input_channel_count = 1\n",
    "    output_channel_count = 1\n",
    "    network = UNet(input_channel_count, output_channel_count, FIRST_LAYER_FILTER_COUNT)\n",
    "    model = network.get_model()\n",
    "    model.load_weights('unet_weights.hdf5')\n",
    "    BATCH_SIZE = 1\n",
    "    threashold = 0.5\n",
    "    previous_whole_time = 0.0\n",
    "    unet_whole_time = 0.0\n",
    "    frame_number = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, img = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_number += 1\n",
    "        previous_masked, previous_lap_time = previous_masking(img)\n",
    "        previous_whole_time += previous_lap_time\n",
    "\n",
    "\n",
    "        timer1 = Timer()\n",
    "\n",
    "        images = np.zeros((1, IMAGE_SIZE, IMAGE_SIZE, 1), np.float32)\n",
    "        image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "        image = image[:, :, np.newaxis]\n",
    "        images[0] = normalize_x(image)\n",
    "        predicted = model.predict(images, BATCH_SIZE)\n",
    "        predicted_mask = cv2.resize(predicted[0], (width, height))\n",
    "        binarized_mask = predicted_mask.copy()\n",
    "        binarized_mask[predicted_mask < threashold] = 0.0\n",
    "        binarized_mask[threashold < predicted_mask] = 1.0\n",
    "        # binarized_mask = cv2.morphologyEx(binarized_mask, cv2.MORPH_OPEN,kernel,iterations=10)\n",
    "        binarized_mask = binarized_mask[:, :, np.newaxis]\n",
    "        masked_img = img * binarized_mask\n",
    "        masked_img = masked_img.astype(np.uint8)\n",
    "        unet_lap_time = timer1.time_elapsed()\n",
    "        unet_whole_time += unet_lap_time\n",
    "\n",
    "        compare_img = cv2.hconcat([img, masked_img])\n",
    "        compare_img = cv2.resize(compare_img, (width, int(height/2)))\n",
    "        cv2.putText(compare_img, 'Original', (10, 350), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 200), thickness=2)\n",
    "        cv2.putText(compare_img, 'U-Net prediction', (522, 350), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 200), thickness=2)\n",
    "        #writer.write(compare_img)\n",
    "        #cv2.imshow('Result', compare_img)\n",
    "\n",
    "        compare_img2 = cv2.hconcat([previous_masked, masked_img])\n",
    "        compare_img2 = cv2.resize(compare_img2, (width, int(height / 2)))\n",
    "        cv2.putText(compare_img2, 'Previous method', (10, 350), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 200), thickness=2)\n",
    "        cv2.putText(compare_img2, 'U-Net prediction', (522, 350), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 200), thickness=2)\n",
    "        writer.write(compare_img2)\n",
    "        cv2.imshow('Result', compare_img2)\n",
    "\n",
    "        # ndarrayはdenormalize後uint8に変換しなければならない\n",
    "\n",
    "        key = cv2.waitKey(1)\n",
    "\n",
    "        if key == 27:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    cv2.waitKey(1)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "    print('Previous : ', end='')\n",
    "    print(previous_whole_time/frame_number)\n",
    "    print('U-Net : ', end='')\n",
    "    print(unet_whole_time/frame_number)\n",
    "\n",
    "    return True\n",
    "\n",
    "movie_masking('okazawa')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37 samples, validate on 10 samples\n",
      "Epoch 1/1000\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 0.6583 - dice_coefficient: 0.3417 - acc: 0.5178 - val_loss: 0.6000 - val_dice_coefficient: 0.4000 - val_acc: 0.7554\n",
      "Epoch 2/1000\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.6286 - dice_coefficient: 0.3714 - acc: 0.6180 - val_loss: 0.5697 - val_dice_coefficient: 0.4303 - val_acc: 0.8143\n",
      "Epoch 3/1000\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.6039 - dice_coefficient: 0.3961 - acc: 0.6736 - val_loss: 0.5387 - val_dice_coefficient: 0.4613 - val_acc: 0.8564\n",
      "Epoch 4/1000\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.5871 - dice_coefficient: 0.4129 - acc: 0.7200 - val_loss: 0.5099 - val_dice_coefficient: 0.4901 - val_acc: 0.8778\n",
      "Epoch 5/1000\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.5718 - dice_coefficient: 0.4282 - acc: 0.7570 - val_loss: 0.4874 - val_dice_coefficient: 0.5126 - val_acc: 0.8843\n",
      "Epoch 6/1000\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.5565 - dice_coefficient: 0.4435 - acc: 0.7886 - val_loss: 0.4702 - val_dice_coefficient: 0.5298 - val_acc: 0.8884\n",
      "Epoch 7/1000\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.5419 - dice_coefficient: 0.4581 - acc: 0.8139 - val_loss: 0.4565 - val_dice_coefficient: 0.5435 - val_acc: 0.8922\n",
      "Epoch 8/1000\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.5299 - dice_coefficient: 0.4701 - acc: 0.8352 - val_loss: 0.4440 - val_dice_coefficient: 0.5560 - val_acc: 0.8995\n",
      "Epoch 9/1000\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.5177 - dice_coefficient: 0.4823 - acc: 0.8486 - val_loss: 0.4295 - val_dice_coefficient: 0.5705 - val_acc: 0.8994\n",
      "Epoch 10/1000\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.5076 - dice_coefficient: 0.4924 - acc: 0.8659 - val_loss: 0.4134 - val_dice_coefficient: 0.5866 - val_acc: 0.9063\n",
      "Epoch 11/1000\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.4934 - dice_coefficient: 0.5066 - acc: 0.8767 - val_loss: 0.4028 - val_dice_coefficient: 0.5972 - val_acc: 0.9092\n",
      "Epoch 12/1000\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.4845 - dice_coefficient: 0.5155 - acc: 0.8889 - val_loss: 0.3958 - val_dice_coefficient: 0.6042 - val_acc: 0.9133\n",
      "Epoch 13/1000\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.4731 - dice_coefficient: 0.5269 - acc: 0.8960 - val_loss: 0.3785 - val_dice_coefficient: 0.6215 - val_acc: 0.9162\n",
      "Epoch 14/1000\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.4642 - dice_coefficient: 0.5358 - acc: 0.9043 - val_loss: 0.3729 - val_dice_coefficient: 0.6271 - val_acc: 0.9188\n",
      "Epoch 15/1000\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.4517 - dice_coefficient: 0.5483 - acc: 0.9126 - val_loss: 0.3567 - val_dice_coefficient: 0.6433 - val_acc: 0.9232\n",
      "Epoch 16/1000\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.4421 - dice_coefficient: 0.5579 - acc: 0.9191 - val_loss: 0.3437 - val_dice_coefficient: 0.6563 - val_acc: 0.9230\n",
      "Epoch 17/1000\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.4322 - dice_coefficient: 0.5678 - acc: 0.9248 - val_loss: 0.3347 - val_dice_coefficient: 0.6653 - val_acc: 0.9230\n",
      "Epoch 18/1000\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.4243 - dice_coefficient: 0.5757 - acc: 0.9290 - val_loss: 0.3213 - val_dice_coefficient: 0.6787 - val_acc: 0.9277\n",
      "Epoch 19/1000\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.4168 - dice_coefficient: 0.5832 - acc: 0.9316 - val_loss: 0.3121 - val_dice_coefficient: 0.6879 - val_acc: 0.9278\n",
      "Epoch 20/1000\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.4076 - dice_coefficient: 0.5924 - acc: 0.9363 - val_loss: 0.3009 - val_dice_coefficient: 0.6991 - val_acc: 0.9292\n",
      "Epoch 21/1000\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.3946 - dice_coefficient: 0.6054 - acc: 0.9422 - val_loss: 0.2940 - val_dice_coefficient: 0.7060 - val_acc: 0.9315\n",
      "Epoch 22/1000\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.3885 - dice_coefficient: 0.6115 - acc: 0.9452 - val_loss: 0.2886 - val_dice_coefficient: 0.7114 - val_acc: 0.9327\n",
      "Epoch 23/1000\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.3819 - dice_coefficient: 0.6181 - acc: 0.9467 - val_loss: 0.2822 - val_dice_coefficient: 0.7178 - val_acc: 0.9342\n",
      "Epoch 24/1000\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.3731 - dice_coefficient: 0.6269 - acc: 0.9499 - val_loss: 0.2746 - val_dice_coefficient: 0.7254 - val_acc: 0.9365\n",
      "Epoch 25/1000\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.3631 - dice_coefficient: 0.6369 - acc: 0.9522 - val_loss: 0.2754 - val_dice_coefficient: 0.7246 - val_acc: 0.9370\n",
      "Epoch 26/1000\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.3552 - dice_coefficient: 0.6448 - acc: 0.9563 - val_loss: 0.2609 - val_dice_coefficient: 0.7391 - val_acc: 0.9398\n",
      "Epoch 27/1000\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.3480 - dice_coefficient: 0.6520 - acc: 0.9582 - val_loss: 0.2589 - val_dice_coefficient: 0.7411 - val_acc: 0.9406\n",
      "Epoch 28/1000\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.3430 - dice_coefficient: 0.6570 - acc: 0.9611 - val_loss: 0.2569 - val_dice_coefficient: 0.7431 - val_acc: 0.9431\n",
      "Epoch 29/1000\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.3276 - dice_coefficient: 0.6724 - acc: 0.9648 - val_loss: 0.2559 - val_dice_coefficient: 0.7441 - val_acc: 0.9434\n",
      "Epoch 30/1000\n",
      "25/37 [===================>..........] - ETA: 0s - loss: 0.3024 - dice_coefficient: 0.6976 - acc: 0.9645"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2164b83d5787>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_unet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[0mplot_loss_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-cc89a47e8eb4>\u001b[0m in \u001b[0;36mtrain_unet\u001b[1;34m()\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[0mNUM_EPOCH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m     history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCH, verbose=1,\n\u001b[1;32m--> 137\u001b[1;33m                         validation_data=(x_validation, y_validation))\n\u001b[0m\u001b[0;32m    138\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'unet_weights.hdf5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# 学習後のU-Netによる予測を行う関数\n",
    "def predict():\n",
    "    import cv2\n",
    "\n",
    "    # test内の画像で予測\n",
    "    X_test, file_names = load_x('Dataset' + os.sep + 'test' + os.sep + 'image')\n",
    "    # X_test, file_names = load_X('testData' + os.sep + 'left_images')\n",
    "\n",
    "    input_channel_count = 1\n",
    "    output_channel_count = 1\n",
    "    first_layer_filter_count = 64\n",
    "    network = UNet(input_channel_count, output_channel_count, first_layer_filter_count)\n",
    "    model = network.get_model()\n",
    "    model.load_weights('unet_weights.hdf5')\n",
    "    BATCH_SIZE = 12\n",
    "    Y_pred = model.predict(X_test, BATCH_SIZE)\n",
    "\n",
    "    for i, y in enumerate(Y_pred):\n",
    "        # testDataフォルダ配下にleft_imagesフォルダを置いている\n",
    "        img = cv2.imread('Dataset' + os.sep + 'test' + os.sep + 'image' + os.sep + file_names[i],0)\n",
    "        # img = cv2.imread('testData' + os.sep + 'left_images' + os.sep + file_names[i])\n",
    "\n",
    "        y = cv2.resize(y, (img.shape[1], img.shape[0]))\n",
    "        y_dn = denormalize_y(y)\n",
    "\n",
    "        cv2.imwrite('prediction' + os.sep + file_names[i], y_dn)\n",
    "        # img_pre = cv2.imread('prediction' + str(i) + '.png')\n",
    "        # img_gt = cv2.imread('testData' + os.sep + 'left_groundTruth' + os.sep + file_names[i])\n",
    "        # img_compare = cv2.hconcat([img_pre, img_gt])\n",
    "        # cv2.imwrite('compare' + str(i) + '.png', img_compare)\n",
    "\n",
    "\n",
    "def masking(path):\n",
    "    import cv2, numpy\n",
    "\n",
    "    img1 = cv2.imread(\"000340.tif\", 0)\n",
    "\n",
    "    img2 = cv2.imread(\"mask.png\", 0)\n",
    "\n",
    "     # img2 = cv2.threshold(img2, 0, 255, cv2.THRESH_OTSU)\n",
    "\n",
    "    masked = cv2.bitwise_and(img1, img1, mask=img2)\n",
    "\n",
    "    cv2.imwrite(path, masked)\n",
    "\n",
    "\n",
    "def create_mask(image):\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "\n",
    "    kernel = np.ones((9, 9), np.uint8)\n",
    "    #tmp_image = cv2.morphologyEx(image, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(7,7)), iterations=5)\n",
    "\n",
    "    tmp_image = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel, iterations=10)\n",
    "\n",
    "\n",
    "    if len(tmp_image.shape) == 3:\n",
    "        tmp_image = cv2.cvtColor(tmp_image, cv2.COLOR_RGB2GRAY)\n",
    "        print(0)\n",
    "\n",
    "    ret, tmp_image = cv2.threshold(tmp_image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "\n",
    "    return tmp_image\n",
    "\n",
    "\n",
    "def create_difference(image1, image2):\n",
    "    import cv2\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    difference = cv2.absdiff(image1, image2)\n",
    "\n",
    "    #cv2.imshow('', difference)\n",
    "    if cv2.waitKey(0) == 'q':\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    mse = mean_squared_error(image1, image2)\n",
    "\n",
    "    return difference, mse\n",
    "\n",
    "\n",
    "# 動画でmaskingを行う関数\n",
    "def movie_masking():\n",
    "    import cv2, os\n",
    "\n",
    "    X_test, file_names = load_x('user_movie' + os.sep + 'hayashi')\n",
    "\n",
    "    input_channel_count = 1\n",
    "    output_channel_count = 1\n",
    "    first_layer_filter_count = 64\n",
    "    network = UNet(input_channel_count, output_channel_count, first_layer_filter_count)\n",
    "    model = network.get_model()\n",
    "    model.load_weights('unet_weights.hdf5')\n",
    "    BATCH_SIZE = 8\n",
    "    Y_pred = model.predict(X_test, BATCH_SIZE)\n",
    "\n",
    "    img = cv2.imread(file_names[0], 0)\n",
    "\n",
    "    for i, y in enumerate(Y_pred):\n",
    "\n",
    "        y = cv2.resize(y, (img.shape[1], img.shape[0]))\n",
    "        y_dn = denormalize_y(y)\n",
    "\n",
    "        cv2.imshow('frame', y_dn)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "history = train_unet()\n",
    "plot_loss_accuracy(history)\n",
    "predict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class DCGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 1000\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # Sample noise and generate a batch of new images\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake images as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            \n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                # Plot the progress\n",
    "                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "                self.save_imgs(epoch)\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        plt.show()\n",
    "        # fig.savefig('./out.png')\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "    #dcgan = DCGAN()\n",
    "    #dcgan.train(epochs=5000, batch_size=32, save_interval=100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
